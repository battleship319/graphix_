version: '3.8'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.0
    ports:
      - "8080:8080"
      - "50051:50051" # For gRPC
    restart: always
    environment:
      QUERY_DEFAULTS_LIMIT: 30
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true' # For local testing, allows unauthenticated access
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      WEAVIATE_GRPC_MAX_MSG_SIZE: 100000000
      # Set the default vectorizer module and enable it
      # DEFAULT_VECTORIZER_MODULE: "text2vec-transformers"
      DEFAULT_VECTORIZER_MODULE: "none"
      ENABLE_MODULES: "text2vec-transformers"
      TRANSFORMERS_INFERENCE_API: "http://t2v-transformers:8080" # Link to the transformer service
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - ./data/weaviate_data:/var/lib/weaviate
  t2v-transformers: # Add this service for the transformer model
    image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1 # You can choose another model here
    environment:
      ENABLE_CUDA: '0' # Set to '1' if you have a CUDA-enabled GPU
    networks:
      - default # Or a custom network if you define one
networks: # Define a network if you don't want to use the default bridge network implicitly
  default:
    driver: bridge